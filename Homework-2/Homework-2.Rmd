---
title: "Homework 2"
subtitle: "ECO395M"
author: "Alice Kemp"
date: "02/22/2022"
output: md_document
---

```{r setup, include=FALSE, echo=FALSE}
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}
librarian::shelf(
  ROSE,
  tidyverse,
  dplyr,
  mosaic,
  haven, 
  stargazer, 
  RColorBrewer,
  here,
  ggmap,
  scales, 
  ggridges,
  viridis,
  gganimate, 
  gifski, 
  future, 
  geosphere,
  maps, 
  airportr, 
  kableExtra,
  rsample,
  caret,
  modelr,
  parallel,
  foreach, 
  glmnet, 
  gmodels)
## color palette brewing
plasma = c("darkorange2", "coral2", "orangered2", "violetred2", "magenta4", "purple3", "mediumblue")
## my theme

my_theme = theme_minimal(base_family = "Arial Narrow", base_size = 12) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold", size = rel(1.7)),
        plot.subtitle = element_text(face = "plain", size = rel(1.0), color = "grey70"),
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "grey70", hjust = 0),
        legend.title = element_text(face = "bold"),
        strip.text = element_text(face = "bold", size = rel(1.1), hjust = 0),
        axis.title = element_text(face = "bold"),
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1))

my_scatter_theme = theme_gray(base_family = "Arial Narrow", base_size = 12) +
  theme(
        plot.title = element_text(face = "bold", size = rel(1.7)),
        plot.subtitle = element_text(face = "plain", size = rel(1.0), color = "grey70"),
        plot.caption = element_text(face = "italic", size = rel(0.7), 
                                    color = "grey70", hjust = 0),
        legend.title = element_text(face = "bold"),
        axis.title = element_text(face = "bold"),
        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),
        axis.title.y = element_text(margin = margin(r = 10), hjust = 1))
here::i_am("R/include.R")
```

# Problem 1: Capmetro UT Visualization
```{r,echo = FALSE, message = FALSE, warning=FALSE}
capmetro_UT = read_csv(here(("data/capmetro_UT_raw.csv")), show_col_types = FALSE) 
capmetro_UT %>%
  mutate(across(day_of_week, factor, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")),
         across(month, factor, levels = c("Sep", "Oct", "Nov"))) %>%
  group_by(hour_of_day, day_of_week, month) %>%
  summarize(avg_boardings = mean(boarding)) %>%
ggplot(aes(x=hour_of_day,y=avg_boardings, color = month)) + 
  geom_line() + 
  facet_wrap(vars(day_of_week),nrow=2) + 
  scale_color_manual(values = c("steelblue1", "royalblue1", "mediumblue")) +
  my_theme + 
  ggtitle("Capmetro UT average boardings",
          subtitle = "Passenger demand is highest on weekdays and peaks during the evening between 4:00 and 6:00 pm") + 
    xlab("Hour of Day") + 
    ylab("Average Boardings")

```


```{r, echo = FALSE, message = FALSE, warning=FALSE}
capmetro_UT %>%
  separate(col = timestamp, into = c("date", "hourwindow"), sep = "\\ ") %>%
  group_by(temperature, weekend) %>%
ggplot(aes(x=temperature,y=boarding, color = weekend)) + 
  geom_point(alpha = 0.6, size = 0.9) + 
  facet_wrap(vars(hour_of_day),nrow=4) + 
  scale_color_manual(values=c("steelblue2","mediumblue")) + 
  my_scatter_theme + 
  ggtitle("Temperature vs. Capmetro UT boardings by hour",
          subtitle = "Passenger demand is higher on weekdays, peaking during evening commute periods. \n Temperature appears to have little impact on number of passenger boardings") + 
  labs(
    x = "Temperature (Â°F)",
    y = "Total Boardings"
  )
```

# Problem 2: Saratoga house prices
## Data
  The dataset used in this report covers housing prices and 15 characteristics for over 1,700 houses in the Saratoga County, Florida residential market in the year 2006. Characteristics include lot size, age, land value, living area, percent of neighborhood that graduated college, bedrooms, fireplaces, bathrooms, rooms, type of heating, type of fuel used for heating, type of sewer system, and whether the property was waterfront, new construction, and had central air conditioning. To better compare performance across models, the categorical variables were converted to binary dummy variables. The data was then normalized in order to build the K-nearest neighbors model. 
   
```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(glmnet)
library(data.table)
library(tidyverse)
library(ggplot2)
library(caret)
library(modelr)
library(rsample)
library(foreach)
library(parallel)
library(purrr)
data(SaratogaHouses)

# normalize data and create dummies
saratoga_norm = fastDummies::dummy_cols(SaratogaHouses, select_columns = c("heating", "fuel", "sewer"))
saratoga_norm = saratoga_norm %>%
  select(-c("heating", "fuel", "sewer")) %>%
  rename(heating_hotair = `heating_hot air`,
        heating_hotwatersteam = `heating_hot water/steam`,
        sewer_publiccommercial = `sewer_public/commercial`) %>%
  mutate(
    waterfront = ifelse(waterfront == "Yes",1,0),
    newConstruction = ifelse(newConstruction == "Yes",1,0),
    centralAir = ifelse(centralAir == "Yes",1,0)
  )
normalize = function(x){
  output = (x - mean(x))/ sd(x)
  return(output)
}

for(col.i in 1:ncol(saratoga_norm)){
  saratoga_norm[,col.i] = normalize(saratoga_norm[,col.i])
}

## Baseline OLS Models
set.seed(5)
saratoga_split = initial_split(saratoga_norm, 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating_hotair + heating_hotwatersteam + heating_electric + fuel_gas + centralAir + fuel_electric + fuel_oil + sewer_septic + sewer_publiccommercial + sewer_none, data=saratoga_train)

lm_big = lm(price ~ (lotSize + age + livingArea + pctCollege + bedrooms + fireplaces + bathrooms + rooms + heating_hotair + heating_hotwatersteam + heating_electric + fuel_gas + centralAir + fuel_electric + fuel_oil + sewer_septic + sewer_publiccommercial + sewer_none + landValue  + newConstruction)^2, data= saratoga_train)
rmse_med = rmse(lm_medium, saratoga_test)
rmse_big = rmse(lm_big, saratoga_test)
  
## CV LASSO Regression ##
listx = list()
for(iter in 1:5){
  set.seed(10)
  saratoga_split = initial_split(saratoga_norm, 0.8)
  saratoga_train = training(saratoga_split)
  saratoga_test = testing(saratoga_split)
  
  xtrain = model.matrix(price ~ .^2, saratoga_train) # include all interactions
  ytrain = saratoga_train$price
  xtest = model.matrix(price ~ .^2, saratoga_test)
  ytest = saratoga_test$price
  
  glmnet_cv = cv.glmnet(xtrain, ytrain, type.measure = "mse", alpha = 1, nfolds = 5) #find best lambda
  lasso_pred = predict(glmnet_cv, s=glmnet_cv$lambda.min, newx=xtest) # predicted vals 
  rmselasso = sqrt(mean((ytest-lasso_pred)^2)) 
  
  listx[[iter]] = data.frame(rmselasso = rmselasso)
}   

## K-fold CV KNN Model ##
# create model over 100 values of k, CV with 10 folds
K_folds = 5
saratoga_folds = crossv_kfold(saratoga_norm, k=K_folds)
k_grid = c(1:100)
cv_grid = foreach(k = k_grid, .combine='rbind') %do% {
  set.seed(10)
  models = purrr::map(saratoga_folds$train, ~ knnreg(price ~ ., data = ., k=k, use.all=FALSE))
  errs = map2_dbl(models, saratoga_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

k_val = as.integer(which.min(cv_grid$err)) # value of k with lowest rmse
rmse_knn = cv_grid$err[k_val] # store rmse 

## Compare RMSE across models
lasso_results = rbindlist(listx) 
rmse_lasso = colMeans(lasso_results)
rmse_compare = data.frame(rbind(rmse_med, rmse_big, rmse_lasso, rmse_knn))
colnames(rmse_compare) = "rmse"
format(rmse_compare, scientific = FALSE)
```

## Methodology
  To analyze the effects of house features on a home's sale price, four models of varying complexity were created. The first two baseline models utilized simple linear OLS regressions of price on a selection of home features, with the "big" OLS model including interaction effects of all included features. The data was also randomized into train/test splits with an 80% training proportion to build and evaluate model performance. The next model utilized a LASSO model for feature selection. All features and their interactions were included, and k-fold cross validation (K = 10) was performed to minimize the risk of overfitting the sample data. Finally, a K-nearest neighbors model was run and iterated over 100 values of K to identify the final model. This KNN model also iterated over K-10 folds for cross validation purposes.    
  
## Technical Notes
  To compare out-of-sample performance across models, the RMSE was calculated for each regression for the test set and an average RMSE was calculated as the mean out-of-sample RMSE across folds for the cross-validated LASSO and KNN models. For the two baseline OLS models, the big model which included most interaction terms performed better than the medium model with an out-of-sample RMSE of approximately 0.649 compared to 0.704. The cross-validated LASSO regression outperformed all other models, yielding an out-of-sample RMSE of approximately 0.554. In the LASSO model, coefficient shrinkage was performed on all possible interactions of all factor levels and features, while the KNN model utilized group patterns in the training data to classify the test data. 
  
## Conclusion
Overall, the cross-validated LASSO model performed the best out of the four regressions, with normalized out-of-sample error of about 0.55. To best predict housing prices based on the variables included in this study, we recommend using the LASSO model. This model not only has a lower error than the other models, but also identifies the most important features and interactions that impact property prices. The features identified in this model are lot size, land value, living area, number of bathrooms, waterfront property, and central air systems. The most important interactions include septic sewer systems based on property age, land value based on new construction status, and living area based on central air status. 

# Problem 3: Classification and retrospective sampling
## Data 
This analysis investigates credit default history of 1,000 customers of a German bank. The data was sampled retrospectively, with the relatively small sample of defaulted loans matched with a similar set of loans that did not default. This sampling resulted in an outsized representation of defaults relative to a random sample of the bank's population of loans. Variables of interest studied in this analysis include loan duration, loan amount, installment amount, consumer age, consumer credit history, loan purpose, and a dummy variable for foreign workers. In this analysis, we attempt to predict a consumer's probability of default based on features deemed statistically significant.


```{r, echo = FALSE, message = FALSE, warning=FALSE}
german_credit = read_csv(here(("data/german_credit_raw.csv")), show_col_types = FALSE)

default_history = german_credit %>%
  mutate(across(history, factor, levels = c("terrible", "poor", "good"))) %>%
  group_by(history) %>%
  summarize(prob_default = sum(Default)/n(), 
            n = n())
ggplot(default_history, aes(x = history, y = prob_default)) + 
  geom_col(fill = "mediumblue", alpha = 0.75) +
  geom_text(aes(label = prob_default %>% round(2), vjust = 2.0), color = "white", family = "Arial Narrow", face = "bold") + 
  my_theme + 
  ggtitle("Credit history vs. probability of loan default in Germany",
          subtitle = "Individuals with poor credit history are likely to default on their loans with <60% probability, compared to \n those with terrible (18%) and good (0.19%) credit")

glm = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, family = "binomial"(link = 'logit'), data = german_credit)
summary(glm)
```
## Methodology
The data was grouped by credit history, with three bins diving consumers by "terrible", "poor", and "good" prior credit history. Then, probability of default was calculated for each group by summing the number of defaulted loans and dividing by the number of consumers per credit group. From the figure shown, we observe that individuals with good credit history have a high probability of defaulting on a loan at approximately 60%, compared to 32% for consumers with "poor" credit and 18% for "terrible" credit history. 

## Evaluation
Looking at the above probabilities, it is clear that the method of retrospective sampling may have introduced some bias in the data analyzed in this report. Upon closer investigation, we find that consumers with "good" credit account for only 9% of the individuals included in the study, while those with "poor" and "terrible" credit make up 62% and 29%, respectively. Therefore, we observe evidence of sampling selection bias in the data points included in this analysis, with a potentially unrepresentative sample fo defaults from "good" credit individuals than would appear in a random sample. To improve this analysis' predictive power in predicting a bank customer's probability of default, we recommend resampling the data to include a balanced panel of individuals across credit histories, or a larger random sample of loans irrespective of default status to gather data that is more representative of the population. 

# Problem 4: Children and hotel reservations
## Data
The data in this analysis includes 45,000 data points from reservations at a major U.S.-based hotel chain. The variable of interest that we are attempting to predict is whether or not a particular booking includes children, based on 21 features including hotel type, meal type, customer demographics, number of adults, repeated guests, and other reservation attributes. 

```{r, echo = FALSE, message = FALSE, warning=FALSE}
hotels = read_csv(here(("data/hotels_dev_raw.csv")), show_col_types = FALSE)
set.seed(13)
hotels_split = initial_split(hotels, 0.8)
hotels_train = training(hotels_split)
hotels_test = testing(hotels_split)

baseline1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, family = "binomial"(link = 'logit'), data = hotels_train)

baseline2 = glm(children ~ . -arrival_date, family = "binomial"(link = 'logit'), data = hotels_train)

x_train = model.matrix(children ~ .-1, data = hotels_train)
y_train = hotels_train$children
x_test = model.matrix(children ~ .-1, data = hotels_test)
y_test = hotels_test$children
lasso_cv = cv.glmnet(x_train, y_train, type.measure = "mse", alpha = 1, nfolds = 10, family = "binomial") 
lasso_predict = predict(lasso_cv, s = 'lambda.min', newx = x_test, alpha = 1, type = "response", family = "binomial"(link='logit'))
rmselasso = sqrt(mean((y_test-lasso_predict)^2)) 

ols1 = rmse(baseline1, hotels_test)
ols2 =rmse(baseline2, hotels_test)
lasso = rmselasso
rmse_hotels = data.frame(rbind(ols1, ols2, lasso))
colnames(rmse_hotels) = "rmse"
rmse_hotels

library(pROC)
proc = roc(y_test ~ lasso_predict, plot = TRUE, print.auc=TRUE, col = "mediumblue", lwd =4, main = "ROC Curve", print.thres = TRUE)


### Model Validation
hotels_val = read_csv(here(("data/hotels_val_raw.csv")))
library(naivebayes)
trctrl = trainControl(method = "cv", number = 20, savePredictions=TRUE)
nb_fit = train(factor(children) ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_val, method = "naive_bayes", trControl=trctrl, tuneLength = 0)
nb_fit
pred = nb_fit$pred
pred$equal = ifelse(pred$pred == pred$obs, 1,0)
eachfold = pred %>%                                        
  group_by(Resample) %>%                         
  summarise_at(vars(equal),                     
               list(Accuracy = mean))  
avg_acc = mean(eachfold$Accuracy)
df = rbind(data.frame(eachfold), avg_acc)
df[21,1] = "Average"
colnames(df) = c("Fold", "Accuracy")
df
```
## Methodology & Evaluation
  First, two baseline OLS models were fit to build a binomial logit regression predicting the probability that a guest booking includes at least one child, using training vs. testing splits to reduce overfitting. Then, a 10-fold cross validated LASSO regression was run on all features to identify the variable most significant in predicting the outcome probability of interest. The LASSO model performed the best out of the three models with a low RMSE of 0.23. A ROC curve was then constructed using the optimal LASSO model to model TFP vs. FPR across varying thresholds. The optimal threshold was t=0.074, yielding an AUC of 0.87. Next, an OLS model was fit to new, unseen hotel data using 20 fold cross validation. The accuracy of the model's predictions for each fold of approximately 250 bookings was calculated, then averaged across all folds for a final accuracy of approximately 71.68%. 
  
  
  
  










